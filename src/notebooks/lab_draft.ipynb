{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, string, random\n",
    "from datetime import datetime\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "sys.path.append('../')\n",
    "from data_utils import MNMv2DataModule\n",
    "from model.unet import LightningSegmentationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configs\n",
    "mnmv2_config   = OmegaConf.load('../configs/datasets/mnmv2.yaml')\n",
    "unet_config    = OmegaConf.load('../configs/model/monai_unet.yaml')\n",
    "trainer_config = OmegaConf.load('../configs/trainer/unet_trainer.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init datamodule\n",
    "datamodule = MNMv2DataModule(\n",
    "    data_dir=mnmv2_config.data_dir,\n",
    "    vendor_assignment=mnmv2_config.vendor_assignment,\n",
    "    batch_size=mnmv2_config.batch_size,\n",
    "    binary_target=mnmv2_config.binary_target,\n",
    "    non_empty_target=mnmv2_config.non_empty_target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "unet = UNet(\n",
    "    spatial_dims=unet_config.spatial_dims,\n",
    "    in_channels=unet_config.in_channels,\n",
    "    out_channels=unet_config.out_channels,\n",
    "    channels=[unet_config.n_filters_init * 2 ** i for i in range(unet_config.depth)],\n",
    "    strides=[2] * (unet_config.depth - 1),\n",
    "    num_res_units=4\n",
    ")\n",
    "\n",
    "model = LightningSegmentationModel(\n",
    "    model=unet,\n",
    "    binary_target=True if unet_config.out_channels == 1 else False,\n",
    "    lr=unet_config.lr,\n",
    "    patience=unet_config.patience,\n",
    "    cfgs={\n",
    "        'dataset': OmegaConf.to_container(mnmv2_config),\n",
    "        'unet': OmegaConf.to_container(unet_config),\n",
    "        'trainer': OmegaConf.to_container(trainer_config)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# infered variable\n",
    "patience = unet_config.patience * 2\n",
    "\n",
    "now = datetime.now()\n",
    "filename = 'mnmv2-' + now.strftime(\"%H-%M_%d-%m-%Y\")\n",
    "\n",
    "# init trainer\n",
    "if trainer_config.logging:\n",
    "    wandb.finish()\n",
    "    logger = WandbLogger(\n",
    "        project=\"lightning\", \n",
    "        log_model=True, \n",
    "        name=filename\n",
    "    )\n",
    "else:\n",
    "    logger = None\n",
    "\n",
    "# trainer\n",
    "trainer = L.Trainer(\n",
    "    limit_train_batches=trainer_config.limit_train_batches,\n",
    "    max_epochs=trainer_config.max_epochs,\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=trainer_config.early_stopping.monitor, \n",
    "            mode=trainer_config.early_stopping.mode, \n",
    "            patience=patience\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=trainer_config.model_checkpoint.dirpath,\n",
    "            filename=filename,\n",
    "            save_top_k=trainer_config.model_checkpoint.save_top_k, \n",
    "            monitor=trainer_config.model_checkpoint.monitor,\n",
    "        )\n",
    "    ],\n",
    "    precision='16-mixed',\n",
    "    gradient_clip_val=0.5,\n",
    "    devices=[7]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjlennartz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20241029_115217-hdmx8psl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jlennartz/lightning/runs/hdmx8psl' target=\"_blank\">mnmv2-11-52_29-10-2024</a></strong> to <a href='https://wandb.ai/jlennartz/lightning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jlennartz/lightning' target=\"_blank\">https://wandb.ai/jlennartz/lightning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jlennartz/lightning/runs/hdmx8psl' target=\"_blank\">https://wandb.ai/jlennartz/lightning/runs/hdmx8psl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | UNet       | 794 K  | train\n",
      "1 | loss  | DiceCELoss | 0      | train\n",
      "---------------------------------------------\n",
      "794 K     Trainable params\n",
      "0         Non-trainable params\n",
      "794 K     Total params\n",
      "3.178     Total estimated model params size (MB)\n",
      "163       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 50/50 [00:08<00:00,  6.08it/s, v_num=8psl]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 50/50 [00:08<00:00,  6.05it/s, v_num=8psl]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../../checkpoints/mnmv2-11-52_29-10-2024.ckpt'\n",
    "\n",
    "load_as_lightning_module = True\n",
    "load_as_pytorch_module = False\n",
    "\n",
    "if load_as_lightning_module:\n",
    "    unet_config    = OmegaConf.load('../configs/model/monai_unet.yaml')\n",
    "    unet = UNet(\n",
    "        spatial_dims=unet_config.spatial_dims,\n",
    "        in_channels=unet_config.in_channels,\n",
    "        out_channels=unet_config.out_channels,\n",
    "        channels=[unet_config.n_filters_init * 2 ** i for i in range(unet_config.depth)],\n",
    "        strides=[2] * (unet_config.depth - 1),\n",
    "        num_res_units=4\n",
    "    )\n",
    "    model = LightningSegmentationModel.load_from_checkpoint(\n",
    "        checkpoint_path,\n",
    "        model=unet,\n",
    "        binary_target=True if unet_config.out_channels == 1 else False,\n",
    "        lr=unet_config.lr,\n",
    "        patience=unet_config.patience,\n",
    "        # cfg=OmegaConf.to_container(unet_config)\n",
    "    )\n",
    "\n",
    "elif load_as_pytorch_module:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model_state_dict = checkpoint['state_dict']\n",
    "    model_state_dict = {k.replace('model.model.', 'model.'): v for k, v in model_state_dict.items() if k.startswith('model.')}\n",
    "    model_config = checkpoint['hyper_parameters']['cfg']\n",
    "\n",
    "    unet = UNet(\n",
    "        spatial_dims=model_config['spatial_dims'],\n",
    "        in_channels=model_config['in_channels'],\n",
    "        out_channels=model_config['out_channels'],\n",
    "        channels=[model_config['n_filters_init'] * 2 ** i for i in range(model_config['depth'])],\n",
    "        strides=[2] * (model_config['depth'] - 1),\n",
    "        num_res_units=4\n",
    "    )\n",
    "\n",
    "    unet.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'data_dir': '../../../../../data/MNM/',\n",
       "  'vendor_assignment': {'train': 'siemens', 'test': 'ge'},\n",
       "  'batch_size': 32,\n",
       "  'binary_target': False,\n",
       "  'non_empty_target': False},\n",
       " 'unet': {'n_filters_init': 16,\n",
       "  'depth': 4,\n",
       "  'spatial_dims': 2,\n",
       "  'in_channels': 1,\n",
       "  'out_channels': 4,\n",
       "  'num_res_units': 4,\n",
       "  'lr': 0.001,\n",
       "  'patience': 5},\n",
       " 'trainer': {'train_transforms': 'global_transforms',\n",
       "  'limit_train_batches': 50,\n",
       "  'max_epochs': 100,\n",
       "  'early_stopping': {'monitor': 'val_loss', 'mode': 'min'},\n",
       "  'model_checkpoint': {'save_top_k': 2,\n",
       "   'dirpath': '../../pre-trained/monai-unets',\n",
       "   'monitor': 'val_loss'},\n",
       "  'logging': True}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cfgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
